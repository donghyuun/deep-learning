{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOV8QGXc/9zMmFqMo9LXn4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghyuun/deep-learning/blob/main/lab_09_1_Neural_Net_for_XOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Logistic Regression"
      ],
      "metadata": {
        "id": "2rdN9DxxVBm_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6m4VymuPzf0",
        "outputId": "6d7cc9e4-5bd2-424a-e346-d14eb48634d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0, Cost: 0.7199724912643433, Accuracy: 0.5\n",
            "Step: 1000, Cost: 0.6931518316268921, Accuracy: 0.25\n",
            "Step: 2000, Cost: 0.6931471824645996, Accuracy: 0.25\n",
            "Step: 3000, Cost: 0.6931471824645996, Accuracy: 0.25\n",
            "Step: 4000, Cost: 0.6931471824645996, Accuracy: 0.25\n",
            "Step: 5000, Cost: 0.6931471824645996, Accuracy: 0.25\n",
            "Step: 6000, Cost: 0.6931471824645996, Accuracy: 0.25\n",
            "Step: 7000, Cost: 0.6931471824645996, Accuracy: 0.25\n",
            "Step: 8000, Cost: 0.6931471824645996, Accuracy: 0.25\n",
            "Step: 9000, Cost: 0.6931471824645996, Accuracy: 0.25\n",
            "Step: 10000, Cost: 0.6931471824645996, Accuracy: 0.25\n",
            "\n",
            "최종 Hypothesis: [[0.49999997]\n",
            " [0.49999997]\n",
            " [0.49999997]\n",
            " [0.5       ]]\n",
            "최종 Accuracy: 0.25\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 데이터\n",
        "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "# 모델 파라미터\n",
        "W = tf.Variable(tf.random.normal([2, 1]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
        "\n",
        "# 모델 정의 (로지스틱 회귀의 가설)\n",
        "def model(X):\n",
        "    return tf.sigmoid(tf.matmul(X, W) + b)\n",
        "\n",
        "# 비용 함수 (크로스 엔트로피 손실)\n",
        "def cost_function(hypothesis, Y):\n",
        "    return tf.reduce_mean(-Y * tf.math.log(hypothesis) - (1 - Y) * tf.math.log(1 - hypothesis))\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "# 정확도 계산\n",
        "def compute_accuracy(hypothesis, Y):\n",
        "    predicted = tf.cast(hypothesis >= 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "# 학습 루프\n",
        "for step in range(10001):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = model(x_data)\n",
        "        cost = cost_function(hypothesis, y_data)\n",
        "\n",
        "    gradients = tape.gradient(cost, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "        accuracy = compute_accuracy(hypothesis, y_data)\n",
        "        print(f\"Step: {step}, Cost: {cost.numpy()}, Accuracy: {accuracy.numpy()}\")\n",
        "\n",
        "# 최종 결과\n",
        "hypothesis_value = model(x_data).numpy()\n",
        "accuracy_value = compute_accuracy(hypothesis_value, y_data).numpy()\n",
        "print(\"\\n최종 Hypothesis:\", hypothesis_value)\n",
        "print(\"최종 Accuracy:\", accuracy_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Net for XOR"
      ],
      "metadata": {
        "id": "YH_kifPVVF3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 데이터\n",
        "x_data = np.array([[0, 0], [1, 1], [1, 0], [0, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "# 모델 파라미터\n",
        "W1 = tf.Variable(tf.random.normal([2, 2]), name='weight1')\n",
        "b1 = tf.Variable(tf.random.normal([2]), name='bias1')\n",
        "W2 = tf.Variable(tf.random.normal([2, 1]), name='weight2')\n",
        "b2 = tf.Variable(tf.random.normal([1]), name='bias2')\n",
        "\n",
        "# 모델 정의\n",
        "def model(X):\n",
        "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "    return hypothesis\n",
        "\n",
        "# 비용 함수 (크로스 엔트로피 손실)\n",
        "def cost_function(hypothesis, Y):\n",
        "    return tf.reduce_mean(-Y * tf.math.log(hypothesis) - (1 - Y) * tf.math.log(1 - hypothesis))\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "# 정확도 계산\n",
        "def compute_accuracy(hypothesis, Y):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "# 학습 루프\n",
        "for step in range(10001):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = model(x_data)\n",
        "        cost = cost_function(hypothesis, y_data)\n",
        "\n",
        "    gradients = tape.gradient(cost, [W1, b1, W2, b2])\n",
        "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "        accuracy = compute_accuracy(hypothesis, y_data)\n",
        "        print(f\"Step: {step}, Cost: {cost.numpy()}, Accuracy: {accuracy.numpy()}\")\n",
        "\n",
        "# 최종 결과\n",
        "hypothesis_value = model(x_data).numpy()\n",
        "accuracy_value = compute_accuracy(hypothesis_value, y_data).numpy()\n",
        "print(\"\\n최종 Hypothesis:\", hypothesis_value)\n",
        "print(\"최종 Accuracy:\", accuracy_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d97CXoFaUp_j",
        "outputId": "a497ca66-7832-4b14-b95f-184691e7a3c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0, Cost: 0.6709762811660767, Accuracy: 0.5\n",
            "Step: 1000, Cost: 0.07519407570362091, Accuracy: 1.0\n",
            "Step: 2000, Cost: 0.03158248960971832, Accuracy: 1.0\n",
            "Step: 3000, Cost: 0.019514741376042366, Accuracy: 1.0\n",
            "Step: 4000, Cost: 0.013973552733659744, Accuracy: 1.0\n",
            "Step: 5000, Cost: 0.010814179666340351, Accuracy: 1.0\n",
            "Step: 6000, Cost: 0.00878175813704729, Accuracy: 1.0\n",
            "Step: 7000, Cost: 0.00736943818628788, Accuracy: 1.0\n",
            "Step: 8000, Cost: 0.006333771161735058, Accuracy: 1.0\n",
            "Step: 9000, Cost: 0.005543551407754421, Accuracy: 1.0\n",
            "Step: 10000, Cost: 0.004921858664602041, Accuracy: 1.0\n",
            "\n",
            "최종 Hypothesis: [[0.00441044]\n",
            " [0.9945678 ]\n",
            " [0.9943967 ]\n",
            " [0.00419005]]\n",
            "최종 Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wide Neural Net fot XOR"
      ],
      "metadata": {
        "id": "WsOnjnIJV3ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 데이터\n",
        "x_data = np.array([[0, 0], [1, 1], [1, 0], [0, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "# 모델 파라미터\n",
        "W1 = tf.Variable(tf.random.normal([2, 10]), name='weight1')  # 입력 크기 2, 첫 번째 레이어 크기 10\n",
        "b1 = tf.Variable(tf.random.normal([10]), name='bias1')  # 첫 번째 레이어 크기\n",
        "layer1 = tf.sigmoid(tf.matmul(x_data, W1) + b1)  # 첫 번째 레이어 출력\n",
        "\n",
        "W2 = tf.Variable(tf.random.normal([10, 1]), name='weight2')  # 두 번째 레이어 크기 1\n",
        "b2 = tf.Variable(tf.random.normal([1]), name='bias2')  # 두 번째 레이어 바이어스\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)  # 최종 출력\n",
        "\n",
        "# 비용 함수 (크로스 엔트로피 손실)\n",
        "def cost_function(hypothesis, Y):\n",
        "    return tf.reduce_mean(-Y * tf.math.log(hypothesis) - (1 - Y) * tf.math.log(1 - hypothesis))\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "# 정확도 계산\n",
        "def compute_accuracy(hypothesis, Y):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "# 학습 루프\n",
        "for step in range(10001):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = tf.sigmoid(tf.matmul(tf.sigmoid(tf.matmul(x_data, W1) + b1), W2) + b2)\n",
        "        cost = cost_function(hypothesis, y_data)\n",
        "\n",
        "    gradients = tape.gradient(cost, [W1, b1, W2, b2])\n",
        "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "        accuracy = compute_accuracy(hypothesis, y_data)\n",
        "        print(f\"Step: {step}, Cost: {cost.numpy()}, Accuracy: {accuracy.numpy()}\")\n",
        "\n",
        "# 최종 결과\n",
        "hypothesis_value = hypothesis.numpy()\n",
        "accuracy_value = compute_accuracy(hypothesis_value, y_data).numpy()\n",
        "print(\"\\n최종 Hypothesis:\", hypothesis_value)\n",
        "print(\"최종 Accuracy:\", accuracy_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbYdqCSWyq_",
        "outputId": "04ca951c-2d16-4469-8bea-adfe5a86451c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0, Cost: 1.8526301383972168, Accuracy: 0.5\n",
            "Step: 1000, Cost: 0.030441250652074814, Accuracy: 1.0\n",
            "Step: 2000, Cost: 0.01097648125141859, Accuracy: 1.0\n",
            "Step: 3000, Cost: 0.00632918905466795, Accuracy: 1.0\n",
            "Step: 4000, Cost: 0.004350658506155014, Accuracy: 1.0\n",
            "Step: 5000, Cost: 0.0032773141283541918, Accuracy: 1.0\n",
            "Step: 6000, Cost: 0.002611033385619521, Accuracy: 1.0\n",
            "Step: 7000, Cost: 0.0021602108608931303, Accuracy: 1.0\n",
            "Step: 8000, Cost: 0.001836362644098699, Accuracy: 1.0\n",
            "Step: 9000, Cost: 0.0015932589303702116, Accuracy: 1.0\n",
            "Step: 10000, Cost: 0.0014045313000679016, Accuracy: 1.0\n",
            "\n",
            "최종 Hypothesis: [[0.00147872]\n",
            " [0.998329  ]\n",
            " [0.9986602 ]\n",
            " [0.00112459]]\n",
            "최종 Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Net for XOR"
      ],
      "metadata": {
        "id": "j7AWKj52ZwbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 데이터\n",
        "x_data = np.array([[0, 0], [1, 1], [1, 0], [0, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "# 모델 파라미터\n",
        "W1 = tf.Variable(tf.random.normal([2, 10]), name='weight1')  # 입력 크기 2, 첫 번째 레이어 크기 10\n",
        "b1 = tf.Variable(tf.random.normal([10]), name='bias1')  # 첫 번째 레이어 크기\n",
        "layer1 = tf.sigmoid(tf.matmul(x_data, W1) + b1)  # 첫 번째 레이어 출력\n",
        "\n",
        "W2 = tf.Variable(tf.random.normal([10, 10]), name='weight2')  # 두 번째 레이어 크기 10\n",
        "b2 = tf.Variable(tf.random.normal([10]), name='bias2')  # 두 번째 레이어 크기\n",
        "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)  # 두 번째 레이어 출력\n",
        "\n",
        "W3 = tf.Variable(tf.random.normal([10, 10]), name='weight3')  # 세 번째 레이어 크기 10\n",
        "b3 = tf.Variable(tf.random.normal([10]), name='bias3')  # 세 번째 레이어 크기\n",
        "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)  # 세 번째 레이어 출력\n",
        "\n",
        "W4 = tf.Variable(tf.random.normal([10, 1]), name='weight4')  # 네 번째 레이어 크기 1\n",
        "b4 = tf.Variable(tf.random.normal([1]), name='bias4')  # 네 번째 레이어 바이어스\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)  # 최종 출력\n",
        "\n",
        "# 비용 함수 (크로스 엔트로피 손실)\n",
        "def cost_function(hypothesis, Y):\n",
        "    return tf.reduce_mean(-Y * tf.math.log(hypothesis) - (1 - Y) * tf.math.log(1 - hypothesis))\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "# 정확도 계산\n",
        "def compute_accuracy(hypothesis, Y):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "# 학습 루프\n",
        "for step in range(10001):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = tf.sigmoid(tf.matmul(tf.sigmoid(tf.matmul(tf.sigmoid(tf.matmul(x_data, W1) + b1), W2) + b2), W3) + b3)\n",
        "        hypothesis = tf.sigmoid(tf.matmul(hypothesis, W4) + b4)\n",
        "        cost = cost_function(hypothesis, y_data)\n",
        "\n",
        "    gradients = tape.gradient(cost, [W1, b1, W2, b2, W3, b3, W4, b4])\n",
        "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3, W4, b4]))\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "        accuracy = compute_accuracy(hypothesis, y_data)\n",
        "        print(f\"Step: {step}, Cost: {cost.numpy()}, Accuracy: {accuracy.numpy()}\")\n",
        "\n",
        "# 최종 결과\n",
        "hypothesis_value = hypothesis.numpy()\n",
        "accuracy_value = compute_accuracy(hypothesis_value, y_data).numpy()\n",
        "print(\"\\n최종 Hypothesis:\", hypothesis_value)\n",
        "print(\"최종 Accuracy:\", accuracy_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRgHMJrnZzcR",
        "outputId": "2c939877-107d-464c-c086-a8981f1111f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0, Cost: 1.0450842380523682, Accuracy: 0.5\n",
            "Step: 1000, Cost: 0.03145857900381088, Accuracy: 1.0\n",
            "Step: 2000, Cost: 0.009913387708365917, Accuracy: 1.0\n",
            "Step: 3000, Cost: 0.005631527863442898, Accuracy: 1.0\n",
            "Step: 4000, Cost: 0.003871897468343377, Accuracy: 1.0\n",
            "Step: 5000, Cost: 0.002926896559074521, Accuracy: 1.0\n",
            "Step: 6000, Cost: 0.0023416224867105484, Accuracy: 1.0\n",
            "Step: 7000, Cost: 0.001945317373611033, Accuracy: 1.0\n",
            "Step: 8000, Cost: 0.001660107751376927, Accuracy: 1.0\n",
            "Step: 9000, Cost: 0.0014454505871981382, Accuracy: 1.0\n",
            "Step: 10000, Cost: 0.0012783341808244586, Accuracy: 1.0\n",
            "\n",
            "최종 Hypothesis: [[1.2398981e-03]\n",
            " [9.9839950e-01]\n",
            " [9.9856538e-01]\n",
            " [8.3490484e-04]]\n",
            "최종 Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}